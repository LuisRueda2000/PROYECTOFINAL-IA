{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62cb6929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD Python: /home/luisrueda/PROYECTO-IA/salidas\n",
      "Archivo que voy a leer: /home/luisrueda/PROYECTO-IA/data/IoT_AquaSensors_Crudo.csv\n",
      "OK, cargado: (74796, 10)\n",
      "Columnas: ['station', 'Date', 'Time', 'NITRATE(PPM)', 'PH', 'AMMONIA(mg/l)', 'TEMP', 'DO', 'TURBIDITY', 'MANGANESE(mg/l)']\n"
     ]
    }
   ],
   "source": [
    "# Importamos utilidades para manejar rutas de forma segura (independiente del SO)\n",
    "from pathlib import Path\n",
    "# Importamos módulos del sistema y pandas para leer el CSV\n",
    "import os, pandas as pd\n",
    "\n",
    "# Ruta ABSOLUTA a la carpeta base del proyecto en WSL\n",
    "BASE = Path(\"/home/luisrueda/PROYECTO-IA\")\n",
    "# Construimos la ruta al CSV uniendo carpetas/archivo de forma segura\n",
    "RUTA = BASE / \"data\" / \"IoT_AquaSensors_Crudo.csv\"\n",
    "\n",
    "# Mostramos el directorio actual donde corre Python (útil para depurar rutas)\n",
    "print(\"CWD Python:\", os.getcwd())\n",
    "# Mostramos la ruta completa del archivo que intentaremos leer\n",
    "print(\"Archivo que voy a leer:\", RUTA)\n",
    "\n",
    "# Verificamos que el archivo exista; si no, cortamos con un error claro\n",
    "assert RUTA.exists(), f\"No se encontró: {RUTA}\"\n",
    "\n",
    "# Leemos el CSV como texto (dtype=str) para tener control total de tipos; \n",
    "# low_memory=False evita inferencias parciales de tipos en archivos grandes\n",
    "df = pd.read_csv(RUTA, dtype=str, low_memory=False)\n",
    "# Confirmamos dimensiones del DataFrame (filas, columnas)\n",
    "print(\"OK, cargado:\", df.shape)\n",
    "# Imprimimos las primeras 12 columnas para inspección rápida\n",
    "print(\"Columnas:\", list(df.columns)[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f16a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estacion      fecha     hora nitrato_ppm  ph amoniaco_mg_l temperatura_c oxigeno_disuelto_mg_l turbidez_ntu manganeso_mg_l\n",
      "station1 01-02-2022 08:00:00        18.3 5.7          0.01          23.2                  11.6         31.7           0.71\n",
      "station1 01-02-2022 08:20:00         3.6 5.1         0.094         23.41                  10.5         18.8           0.62\n",
      "station1 01-02-2022 08:40:00        13.1 5.5          0.06         23.63                  10.3         23.2           0.73\n",
      "station1 01-02-2022 09:00:00        18.1 5.2         0.018         23.64                   9.4         26.7           0.64\n",
      "station1 01-02-2022 09:20:00        10.8 5.2         0.038         23.81                   8.8         19.5           0.68\n",
      "station1 01-02-2022 09:40:00         1.8   5         0.039         23.85                   7.3         18.8           0.67\n",
      "station1 01-02-2022 10:00:00          22 5.3         0.057          24.1                  11.5           32           0.63\n",
      "station1 01-02-2022 10:20:00        17.7 5.9         0.056         24.21                   9.6         20.5            0.7\n",
      "station1 01-02-2022 10:40:00        10.7 5.8         0.065         24.31                  10.9         30.3           0.64\n",
      "station1 01-02-2022 11:00:00         1.4   5         0.006         24.35                     6         31.9           0.68\n",
      "station1 01-02-2022 11:20:00         8.4 5.4         0.013          24.6                   9.5         30.2           0.64\n",
      "station1 01-02-2022 11:40:00        11.4 5.4         0.061         24.67                   7.6         29.8           0.61\n",
      "station1 01-02-2022 12:00:00        20.5   5         0.013         24.69                   6.8         33.2            0.6\n",
      "station1 01-02-2022 12:20:00        12.5 5.8         0.053          24.7                   6.1         29.1           0.72\n",
      "station1 01-02-2022 12:40:00           5   6         0.048         25.13                   7.9         21.1           0.72\n",
      "station1 01-02-2022 13:00:00          25   5         0.092         25.15                   6.8         29.3           0.66\n",
      "station1 01-02-2022 13:20:00        27.5 5.6         0.089         25.38                    12         26.7           0.71\n",
      "station1 01-02-2022 13:40:00         0.9 5.5         0.008         25.82                  10.3         24.6           0.68\n",
      "station1 01-02-2022 14:00:00        29.1 5.3         0.089         25.95                   8.8         22.4           0.64\n",
      "station1 01-02-2022 14:20:00         3.9 5.4         0.045         25.86                   6.7         22.1           0.72\n"
     ]
    }
   ],
   "source": [
    "# Paso 1. Normalización y renombrado de variables\n",
    "#Se realizó una estandarización inicial de la estructura del conjunto de datos para garantizar coherencia semántica \n",
    "# y facilitar su tratamiento posterior. Primero, se homogenizaron los nombres de las columnas convirtiéndolos a \n",
    "# minúsculas y eliminando espacios en blanco; esto evita ambigüedades y fallos al referenciar variables en el\n",
    "# código. Luego, se aplicó un mapeo a nombres canónicos en español, alineados con la terminología del dominio \n",
    "# acuícola y los objetivos del estudio. En particular: station→estacion, date→fecha, time→hora, temp→temperatura_c,\n",
    "# do→oxigeno_disuelto_mg_l, turbidity→turbidez_ntu, ammonia(mg/l)→amoniaco_mg_l, nitrate(ppm)→nitrato_ppm, \n",
    "# manganese(mg/l)→manganeso_mg_l. Este paso mejora la legibilidad, reduce errores en las etapas de limpieza/transformación \n",
    "# y deja un esquema de variables consistente para reproducibilidad y trazabilidad del análisis.\n",
    "\n",
    "df = df.copy()# Hago una copia para no modificar el df original por accidente\n",
    "df.columns = [c.strip().lower() for c in df.columns] # ← quita espacios y pasa nombres de columnas a minúsculas\n",
    "# Mapa típico de las columnas\n",
    "renombrar_es = {\n",
    "    \"station\":        \"estacion\",\n",
    "    \"date\":           \"fecha\",\n",
    "    \"time\":           \"hora\",\n",
    "    \"nitrate(ppm)\":   \"nitrato_ppm\",\n",
    "    \"ph\":             \"ph\",\n",
    "    \"ammonia(mg/l)\":  \"amoniaco_mg_l\",\n",
    "    \"temp\":           \"temperatura_c\",\n",
    "    \"do\":             \"oxigeno_disuelto_mg_l\",\n",
    "    \"turbidity\":      \"turbidez_ntu\",\n",
    "    \"manganese(mg/l)\":\"manganeso_mg_l\",\n",
    "}\n",
    "df = df.rename(columns=renombrar_es)  # ← aplica el diccionario: solo cambia las columnas que existan##\n",
    "print(df.head(20).to_string(index=False))# Ver las primeras 20 filas como tabla en consola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07f81c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nitrato_ppm': 'float64',\n",
       " 'ph': 'float64',\n",
       " 'amoniaco_mg_l': 'float64',\n",
       " 'temperatura_c': 'float64',\n",
       " 'oxigeno_disuelto_mg_l': 'float64',\n",
       " 'turbidez_ntu': 'float64',\n",
       " 'manganeso_mg_l': 'float64'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PASO 2 — Convertir a numérico (coma → punto)\n",
    "#Define qué columnas deben ser numéricas (parámetros de agua)\n",
    "#Recorre solo las que realmente existan en el DataFrame.\n",
    "#Estandariza decimales: cambia coma por punto (ej. 7,2 → 7.2).\n",
    "#Convierte cada columna a tipo numérico con pd.to_numeric(...).\n",
    "#Controla errores: si hay texto u otros valores inválidos, los convierte a NaN (no revienta el proceso).\n",
    "#Resultado: esas variables quedan en formato float/int, listas para cálculos, limpieza por rangos e imputación.\n",
    "\n",
    "import numpy as np                     # ← usamos NumPy por si luego necesitamos operaciones numéricas/NaN\n",
    "\n",
    "# ← lista de columnas que QUEREMOS que sean numéricas\n",
    "num_cols = [\"nitrato_ppm\",\"ph\",\"amoniaco_mg_l\",\"temperatura_c\",\n",
    "            \"oxigeno_disuelto_mg_l\",\"turbidez_ntu\",\"manganeso_mg_l\"]\n",
    "\n",
    "# ← recorremos cada nombre de columna en la lista\n",
    "for col in num_cols:\n",
    "    if col in df.columns:              # ← solo actuamos si la columna existe en el DataFrame\n",
    "        df[col] = pd.to_numeric(       # ← convierte a tipo numérico (float / int)\n",
    "            df[col]                   #    toma la columna\n",
    "              .astype(str)            #    la convierte a string (por si venía mezclada)\n",
    "              .str.replace(\",\", \".\",  #    cambia coma decimal por punto (p. ej., \"7,2\" → \"7.2\")\n",
    "                            regex=False),\n",
    "            errors=\"coerce\"            # ← si hay texto inválido, lo convierte a NaN en vez de dar error\n",
    "        )\n",
    "\n",
    "# ← pequeña inspección: muestra el tipo final de cada columna convertida\n",
    "{c: str(df[c].dtype) for c in num_cols if c in df.columns}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d48cb236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46056/457467362.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[\"marca_tiempo\"] = pd.to_datetime(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estacion</th>\n",
       "      <th>marca_tiempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Station2</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Station2</td>\n",
       "      <td>2022-02-01 00:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Station2</td>\n",
       "      <td>2022-02-01 00:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Station2</td>\n",
       "      <td>2022-02-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Station2</td>\n",
       "      <td>2022-02-01 01:20:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   estacion        marca_tiempo\n",
       "0  Station2 2022-02-01 00:00:00\n",
       "1  Station2 2022-02-01 00:20:00\n",
       "2  Station2 2022-02-01 00:40:00\n",
       "3  Station2 2022-02-01 01:00:00\n",
       "4  Station2 2022-02-01 01:20:00"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PASO 3 — marca_tiempo (fecha+hora) y orden\n",
    "#Construye una marca temporal única (marca_tiempo) combinando fecha y hora.\n",
    "#Parsea ese texto a tipo datetime; si no puede, lo deja como NaT (valor de fecha/hora faltante).\n",
    "#Usa dayfirst=True, asumiendo formato DD/MM/AAAA (útil en datos locales).\n",
    "#Si faltara la columna estacion, la crea como \"desconocida\" para poder agrupar.\n",
    "#Ordena el dataset por estacion y marca_tiempo, dejando la tabla lista para análisis temporal \n",
    "# (lags, ventanas móviles, detección de caídas bruscas, etc.).\n",
    "def construir_ts(r):\n",
    "    f = str(r.get(\"fecha\", \"\") or \"\")   # ← toma 'fecha'; si no existe o viene None, usa \"\"\n",
    "    h = str(r.get(\"hora\", \"\")  or \"\")   # ← toma 'hora'; si no existe o viene None, usa \"\"\n",
    "    # ← si hay 'hora' válida, concatena \"fecha hora\"; si no, devuelve solo 'fecha'\n",
    "    return f\"{f} {h}\" if h and h.lower() not in (\"nan\", \"none\") else f\n",
    "\n",
    "# ← crea columna 'marca_tiempo' convirtiendo texto a datetime\n",
    "#    errors='coerce' pone NaT cuando no puede parsear\n",
    "#    dayfirst=True interpreta DD/MM/YYYY como día-primero (ajústalo si usas MM/DD/YYYY)\n",
    "df[\"marca_tiempo\"] = pd.to_datetime(\n",
    "    df.apply(construir_ts, axis=1),\n",
    "    errors=\"coerce\",\n",
    "    infer_datetime_format=True,\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "# ← si no existe columna 'estacion', créala para poder agrupar/ordenar\n",
    "if \"estacion\" not in df.columns:\n",
    "    df[\"estacion\"] = \"desconocida\"\n",
    "\n",
    "# ← ordena cronológicamente por estación y marca de tiempo; reinicia índice limpio\n",
    "df = df.sort_values([\"estacion\", \"marca_tiempo\"]).reset_index(drop=True)\n",
    "\n",
    "# ← vista rápida para verificar\n",
    "df[[\"estacion\", \"marca_tiempo\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a89fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ph': 0,\n",
       " 'temperatura_c': 0,\n",
       " 'oxigeno_disuelto_mg_l': 0,\n",
       " 'turbidez_ntu': 0,\n",
       " 'amoniaco_mg_l': 0,\n",
       " 'nitrato_ppm': 0,\n",
       " 'manganeso_mg_l': 0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PASO 4 — depura valores imposibles\n",
    "#Define rangos plausibles para cada parámetro (pH, temperatura, DO, turbidez, amoníaco, nitrato, manganeso) \n",
    "# basados en límites físico-químicos razonables para acuicultura.\n",
    "#Revisa cada columna y crea una máscara que detecta lecturas no nulas fuera de esos rangos \n",
    "# (posibles errores de sensor, unidades mal registradas o outliers imposibles).\n",
    "#Sustituye esas lecturas fuera de rango por NaN para no sesgar análisis/modelos; estos NaN se imputarán en el Paso 5.\n",
    "#Registra cuántos valores fueron reemplazados por variable en el dict reemplazos, lo que te da un resumen de calidad y trazabilidad de la limpieza.\n",
    "# Diccionario: para cada variable, (mínimo_aceptable, máximo_aceptable)\n",
    "rangos = {\n",
    "    \"ph\":                     (0, 14),    # pH físico-químico típico\n",
    "    \"temperatura_c\":         (0, 50),    # °C plausibles en acuicultura\n",
    "    \"oxigeno_disuelto_mg_l\": (0, 20),    # mg/L\n",
    "    \"turbidez_ntu\":          (0, 1000),  # NTU (muy amplio)\n",
    "    \"amoniaco_mg_l\":         (0, 10),    # mg/L\n",
    "    \"nitrato_ppm\":           (0, 200),   # ppm\n",
    "    \"manganeso_mg_l\":        (0, 10),    # mg/L\n",
    "}\n",
    "\n",
    "reemplazos = {}  # ← aquí guardaremos cuántos valores fueron marcados como NaN por variable\n",
    "\n",
    "# Recorremos cada variable y su par (mín, máx)\n",
    "for col, (lo, hi) in rangos.items():\n",
    "    if col in df.columns:  # ← solo si la columna existe en el DataFrame\n",
    "        # Máscara: valores no nulos y que están fuera del rango permitido\n",
    "        mask = df[col].notna() & ((df[col] < lo) | (df[col] > hi))\n",
    "        # Guardamos cuántos se van a reemplazar (diagnóstico)\n",
    "        reemplazos[col] = int(mask.sum())\n",
    "        # Ponemos NaN en los fuera de rango (se imputarán en el PASO 5)\n",
    "        df.loc[mask, col] = np.nan\n",
    "\n",
    "# Mostrar resumen: cuántos valores se marcaron como NaN por estar fuera de rango\n",
    "reemplazos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcd5aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN antes (%): {'nitrato_ppm': '0.0', 'ph': '0.0', 'amoniaco_mg_l': '0.0', 'temperatura_c': '0.0', 'oxigeno_disuelto_mg_l': '0.0', 'turbidez_ntu': '0.0', 'manganeso_mg_l': '0.0'}\n",
      "NaN después (%): {'nitrato_ppm': '0.0', 'ph': '0.0', 'amoniaco_mg_l': '0.0', 'temperatura_c': '0.0', 'oxigeno_disuelto_mg_l': '0.0', 'turbidez_ntu': '0.0', 'manganeso_mg_l': '0.0'}\n",
      "PASO 5 OK → nulos restantes en numéricas: 0\n"
     ]
    }
   ],
   "source": [
    "# ===== PASO 5: Imputación de faltantes por estación (mediana; respaldo global) =====\n",
    "#Objetivo: rellenar los valores faltantes (NaN) en las variables numéricas respetando el contexto de cada estación.\n",
    "#Cómo lo hace: para cada columna numérica, calcula la mediana por estacion y reemplaza los NaN de esa estación con su mediana.\n",
    "#Si una estación no tiene datos suficientes para calcular mediana, usa la mediana global de la columna como respaldo.\n",
    "#Diagnóstico: imprime el porcentaje de NaN antes y después para que verifiques el impacto de la imputación.\n",
    "#Salida: produce df_limpio (y luego lo asigna a df) con los huecos cubiertos, dejando el dataset coherente y completo para los pasos siguientes.\n",
    "#Notas: si alguna columna queda con NaN es porque toda la columna estaba vacía (no hay información para imputar).\n",
    "import numpy as np  # ← para manejar NaN con seguridad\n",
    "\n",
    "# Lista de columnas numéricas a imputar (se filtran solo las que existan en el df)\n",
    "cols_num = [c for c in [\n",
    "    \"nitrato_ppm\",\"ph\",\"amoniaco_mg_l\",\"temperatura_c\",\n",
    "    \"oxigeno_disuelto_mg_l\",\"turbidez_ntu\",\"manganeso_mg_l\"\n",
    "] if c in df.columns]\n",
    "\n",
    "# (Opcional) Diagnóstico antes de imputar: porcentaje de NaN por columna\n",
    "print(\"NaN antes (%):\", (df[cols_num].isna().mean().round(3)*100).astype(str).to_dict())\n",
    "\n",
    "def imputar_por_estacion(dataframe, columnas):\n",
    "    \"\"\"Rellena NaN usando la mediana por 'estacion'; si no hay datos en esa estación, usa la mediana global.\"\"\"\n",
    "    df_tmp = dataframe.copy()  # ← trabajamos sobre una copia para no alterar el original\n",
    "    for col in columnas:       # ← recorremos cada columna numérica a imputar\n",
    "        med_global = df_tmp[col].median(skipna=True)  # ← mediana global de respaldo para esta columna\n",
    "        # Aplicamos por estación: calcula la mediana de la serie en esa estación y rellena sus NaN\n",
    "        df_tmp[col] = df_tmp.groupby(\"estacion\")[col].transform(\n",
    "            lambda s: s.fillna(s.median(skipna=True) if not np.isnan(s.median(skipna=True)) else med_global)\n",
    "        )\n",
    "    return df_tmp  # ← devolvemos el dataframe con NaN imputados\n",
    "\n",
    "# Ejecutamos la imputación sobre las columnas numéricas seleccionadas\n",
    "df_limpio = imputar_por_estacion(df, cols_num)\n",
    "\n",
    "# (Opcional) Diagnóstico después de imputar: debería tender a 0% si había datos suficientes\n",
    "print(\"NaN después (%):\", (df_limpio[cols_num].isna().mean().round(3)*100).astype(str).to_dict())\n",
    "\n",
    "# Conteo total de NaN restantes en columnas numéricas (idealmente 0)\n",
    "total_nulos = int(df_limpio[cols_num].isna().sum().sum())\n",
    "print(f\"PASO 5 OK → nulos restantes en numéricas: {total_nulos}\")\n",
    "\n",
    "# Importante: seguir trabajando desde df_limpio a partir de aquí\n",
    "df = df_limpio  # ← actualizamos 'df' para los siguientes pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dddbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASO 6 OK → columnas escaladas añadidas: ['ph_z']\n",
      "Forma final: (74796, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisrueda/PROYECTO-IA/.venv/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1216: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/home/luisrueda/PROYECTO-IA/.venv/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1406: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n"
     ]
    }
   ],
   "source": [
    "# ===== PASO 6: Transformación con escalado robusto (añade columnas *_z) =====\n",
    "#Objetivo: normalizar la escala de las variables numéricas para que sean comparables y robustas a outliers.\n",
    "#Cómo lo hace: aplica RobustScaler (centra por mediana y escala por IQR). \n",
    "#No reemplaza tus columnas originales: agrega nuevas con sufijo _z (ej.: ph_z, temperatura_c_z).\n",
    "#Entrada: el df ya imputado en el Paso 5 (sin NaN en numéricas).\n",
    "#Salida: un df_transformado (que luego reasignas a df) con todas las columnas originales + las versiones escaladas _z.\n",
    "#Para qué sirve: muchos modelos (especialmente de detección de anomalías y distancia) funcionan mejor cuando las\n",
    "# features están en la misma escala y sin sesgo por valores extremos.\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler   # ← importamos el escalador robusto (menos sensible a outliers)\n",
    "\n",
    "# ← columnas numéricas que queremos escalar (ajusta si falta alguna en tu df)\n",
    "cols_num = [c for c in [\n",
    "    \"nitrato_ppm\",\"ph\",\"amoniaco_mg_l\",\"temperatura_c\",\n",
    "    \"oxigeno_disuelto_mg_l\",\"turbidez_ntu\",\"manganeso_mg_l\"\n",
    "] if c in df.columns]\n",
    "\n",
    "escalador = RobustScaler()                       # ← creamos el objeto escalador\n",
    "Xz = escalador.fit_transform(df[cols_num])       # ← ajusta con tus datos y transforma → matriz numpy escalada\n",
    "\n",
    "df_transformado = df.copy()                      # ← trabajamos sobre una copia para conservar las columnas originales\n",
    "for i, c in enumerate(cols_num):                 # ← recorremos cada columna numérica en el mismo orden\n",
    "    df_transformado[c + \"_z\"] = Xz[:, i]         # ← añadimos la versión escalada con sufijo \"_z\"\n",
    "\n",
    "print(\"PASO 6 OK → columnas escaladas añadidas:\", [c + \"_z\" for c in cols_num][:8])  # ← muestra algunas nuevas\n",
    "print(\"Forma final:\", df_transformado.shape)     # ← filas y columnas después de agregar las escaladas\n",
    "\n",
    "# (Opcional) a partir de aquí seguimos usando df_transformado como dataset transformado\n",
    "df = df_transformado                              # ← actualizamos df para pasos posteriores (si los hubiera)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
